version: "3.3"
services:
  #This docker-compose file allows for the spinup of a locally built ooba web client and spin up of a standard qdrant vector database server.
  #Note that both containers share a docker network on which to talk. you will need to create that docker network prior to useing this docker-compose
  #Also note that you need to copy/move this to the root folder of the app if you want to start qdrant with it.
  #Lastly, you'll want to create a .env file in the root directory that defines all of the ports below if the defaults don't work for you.
  #Note this assumes you have an nvidia gpu, see below for config

  text-generation-webui:
    container_name: ooba
    build:
      context: .
      args:
        # specify which cuda version your card supports: https://developer.nvidia.com/cuda-gpus
        TORCH_CUDA_ARCH_LIST: ${TORCH_CUDA_ARCH_LIST:-8.0}
        WEBUI_VERSION: ${WEBUI_VERSION:-HEAD}

    env_file: .env

    user: root
    ports:
      - "${HOST_PORT:-7860}:${CONTAINER_PORT:-7860}"
      - "${HOST_API_PORT:-5000}:${CONTAINER_API_PORT:-5000}"
      - "${HOST_API_STREAM_PORT:-5005}:${CONTAINER_API_STREAM_PORT:-5005}"
    stdin_open: true
    tty: true
    volumes:
      - ${CHARACTERS_DIR:-./characters}:/app/characters
      - ${MODELS_DIR:-./models}:/app/models
      - ${QDRANT_DIR:-./extensions/long_term_memory/qdrant}:/app/qdrant
      - ./extensions:/app/extensions
      - ./loras:/app/loras
      - ./presets:/app/presets
      - ./prompts:/app/prompts
      - ./softprompts:/app/softprompts
      - ./training:/app/training
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    networks:
      - shared_network

  qdrant:
    container_name: qdrant
    image: qdrant/qdrant
    ports:
      - "6333:6333"
    volumes:
          - ${QDRANT_DIR:-./extensions/long_term_memory/qdrant}:/app/qdrant
    networks:
      - shared_network

networks:
  shared_network:
    external: true

